{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monikayyy/crowd-enVent-modeling/blob/master/DeBlur_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVfGNSDq8yeT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIQoRyk37iAv"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python\n",
        "!pip install natsort\n",
        "!pip install torchmetrics\n",
        "!pip install lmdb\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "id": "RnXdEzrWPvbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWnbcFmZf4qh"
      },
      "source": [
        "# Load TransformerPerceptualLoss module\n",
        "\n",
        "Reference: Image Deblurring by Exploring In-depth Properties of Transformer\n",
        "\n",
        "- changes in double_mae_model: remove qv_scale\n",
        "- changes in deblurloss: add a check for dict extraction from checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdE66cTH6R4c"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/erfect2020/TransformerPerceptualLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqRuREJviFMA"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/TransformerPerceptualLoss /content/drive/MyDrive/REDS/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJuNGj3EhCT4"
      },
      "source": [
        "# Fetch data\n",
        "- REDS dataset\n",
        "  - train\n",
        "  - val\n",
        "  - test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pNRHvW8s9q9"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "\n",
        "repo_id = \"snah/REDS\"\n",
        "repo_type = \"dataset\"\n",
        "\n",
        "files_to_download = [\n",
        "    \"train_blur.zip\",\n",
        "    \"train_sharp.zip\",\n",
        "    \"val_blur.zip\",\n",
        "    \"val_sharp.zip\",\n",
        "    \"test_blur.zip\"\n",
        "]\n",
        "\n",
        "downloaded_files = []\n",
        "for file_path in files_to_download:\n",
        "    print(f\"Downloading: {file_path} from {repo_id}\")\n",
        "    try:\n",
        "        local_path = hf_hub_download(\n",
        "            repo_id=repo_id,\n",
        "            filename=file_path,\n",
        "            repo_type=repo_type,\n",
        "            # token=True, # Use if you logged in via notebook_login() or HF_TOKEN secret\n",
        "            local_dir='.',          # Optional: Download directly to current dir (./content/) instead of cache\n",
        "            local_dir_use_symlinks=False # Recommended with local_dir='.' to avoid symlinks\n",
        "        )\n",
        "        downloaded_files.append(local_path)\n",
        "        print(f\"Downloaded to: {local_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {file_path}: {e}\")\n",
        "\n",
        "print(\"\\nFinished download attempts.\")\n",
        "print(\"Downloaded file paths:\", downloaded_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RODX5qhDiOJc"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/train_blur.zip /content/drive/MyDrive/REDS/\n",
        "!cp -r /content/train_sharp.zip /content/drive/MyDrive/REDS/\n",
        "!cp -r /content/val_blur.zip /content/drive/MyDrive/REDS/\n",
        "!cp -r /content/val_sharp.zip /content/drive/MyDrive/REDS/\n",
        "!cp -r /content/test_blur.zip /content/drive/MyDrive/REDS/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTViGisbt1pp"
      },
      "source": [
        "# Dataset Preparation\n",
        "- select pairs of blur and sharp image files from train and val\n",
        "- 24000 pairs in train\n",
        "- 3000 pairs in val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2G-Qp5AvDMy"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import random\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import glob\n",
        "\n",
        "class PairedZipDataset(Dataset):\n",
        "    def __init__(self, blur_zip_path, sharp_zip_path, transform=None):\n",
        "        self.blur_zip = zipfile.ZipFile(blur_zip_path, 'r')\n",
        "        self.sharp_zip = zipfile.ZipFile(sharp_zip_path, 'r')\n",
        "        self.transform = transform\n",
        "        self.blur_files = sorted([f for f in self.blur_zip.namelist() if not f.endswith('/')])\n",
        "        self.sharp_files = sorted([f for f in self.sharp_zip.namelist() if not f.endswith('/')])\n",
        "        self.blur_folders = self.group_by_IMG_folder(self.blur_files)\n",
        "        self.sharp_folders = self.group_by_IMG_folder(self.sharp_files)\n",
        "        self.paired_files = []\n",
        "        paired_count = 0\n",
        "\n",
        "        for folder in self.blur_folders.keys():\n",
        "            if folder in self.sharp_folders.keys():\n",
        "                blur_imgs = self.blur_folders[folder]\n",
        "                sharp_imgs = self.sharp_folders[folder]\n",
        "                if not blur_imgs or not blur_imgs:\n",
        "                  continue\n",
        "\n",
        "                blur_map = {os.path.splitext(os.path.basename(p))[0]: p for p in blur_imgs}\n",
        "                sharp_map = {os.path.splitext(os.path.basename(p))[0]: p for p in sharp_imgs}\n",
        "                common_filenames = sorted(list(blur_map.keys() & sharp_map.keys())) # Intersection of keys\n",
        "\n",
        "                if not common_filenames:\n",
        "                    continue\n",
        "\n",
        "                # folder_pairs = []\n",
        "                for fname in common_filenames:\n",
        "                    self.paired_files.append((blur_map[fname], sharp_map[fname]))\n",
        "                    paired_count += 1\n",
        "        print(f\"Total paired files added: {paired_count}\")\n",
        "        if not self.paired_files:\n",
        "            print(f\"Warning: No paired files found after matching filenames. Check filenames and structure in zip files.\")\n",
        "\n",
        "    def group_by_IMG_folder(self, file_paths):\n",
        "        folder_dict = {}\n",
        "        for file_path in file_paths:\n",
        "            folder = os.path.basename(os.path.dirname(file_path))\n",
        "            folder_dict.setdefault(folder, []).append(file_path)\n",
        "        return folder_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paired_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        blur_file, sharp_file = self.paired_files[idx]\n",
        "\n",
        "\n",
        "        blur_img = self.load_image_from_zip(self.blur_zip, blur_file)\n",
        "        sharp_img = self.load_image_from_zip(self.sharp_zip, sharp_file)\n",
        "\n",
        "        if self.transform:\n",
        "            blur_img = self.transform(blur_img)\n",
        "            sharp_img = self.transform(sharp_img)\n",
        "\n",
        "        return blur_img, sharp_img\n",
        "\n",
        "    def load_image_from_zip(self, zip_file, file):\n",
        "        with zip_file.open(file) as file:\n",
        "            img_data = file.read()\n",
        "            img = Image.open(BytesIO(img_data)).convert('RGB')\n",
        "        return img\n",
        "\n",
        "    def load_image_from_idx(self, idx):\n",
        "        blur_img, sharp_img = self.paired_files[idx]\n",
        "\n",
        "        blur = self.load_image_from_zip(self.blur_zip, blur_img)\n",
        "        sharp = self.load_image_from_zip(self.sharp_zip, sharp_img)\n",
        "        return blur, sharp\n",
        "\n",
        "    def close(self):\n",
        "        try:\n",
        "            if self.blur_zip:\n",
        "                self.blur_zip.close()\n",
        "        except Exception as e:\n",
        "            print(f\"Error closing blur zip: {e}\")\n",
        "        try:\n",
        "            if self.sharp_zip:\n",
        "                self.sharp_zip.close()\n",
        "        except Exception as e:\n",
        "            print(f\"Error closing sharp zip: {e}\")\n",
        "\n",
        "    def __del__(self):\n",
        "        self.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parameters"
      ],
      "metadata": {
        "id": "sE3DUnpi2gNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "BLUR_ZIP_PATH = '/content/drive/MyDrive/REDS/train_blur.zip'\n",
        "SHARP_ZIP_PATH = '/content/drive/MyDrive/REDS/train_sharp.zip'\n",
        "VAL_BLUR_ZIP_PATH = '/content/drive/MyDrive/REDS/val_blur.zip'\n",
        "VAL_SHARP_ZIP_PATH = '/content/drive/MyDrive/REDS/val_sharp.zip'\n",
        "TEST_BLUR_ZIP_PATH = '/content/drive/MyDrive/REDS/test_blur.zip'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/REDS/mae_vit_output'\n",
        "\n",
        "#Pretrained MAE vision transformer for feature extraction\n",
        "PRETRAINED_WEIGHTS_PATH = '/content/drive/MyDrive/REDS/pytorch_model.bin'\n",
        "\n",
        "IMG_SIZE = 224\n",
        "VAL_BATCH_SIZE = 15\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "LR_DECAY_EPOCHS = 40\n",
        "LR_FINAL = 2e-5\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 0.05\n",
        "NUM_WORKERS = 1\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "id": "6LT7hvU52XID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load modules from TransformerPerceptualLoss for feature extraction and perceptual loss calculation\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/REDS/TransformerPerceptualLoss')\n",
        "sys.path.append('/content/drive/MyDrive/REDS/TransformerPerceptualLoss/models')\n",
        "sys.path.append('/content/drive/MyDrive/REDS/TransformerPerceptualLoss/loss')\n",
        "sys.path.append('/content/drive/MyDrive/REDS/TransformerPerceptualLoss/utils')\n",
        "print(sys.path)"
      ],
      "metadata": {
        "id": "RfFWo4nM2yhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    worker_info = torch.utils.data.get_worker_info()\n",
        "    dataset = worker_info.dataset\n",
        "    if hasattr(dataset, '_open_zips'):\n",
        "         dataset._open_zips()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = PairedZipDataset(\n",
        "    blur_zip_path=BLUR_ZIP_PATH,\n",
        "    sharp_zip_path=SHARP_ZIP_PATH,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "val_dataset = PairedZipDataset(\n",
        "    blur_zip_path=VAL_BLUR_ZIP_PATH,\n",
        "    sharp_zip_path=VAL_SHARP_ZIP_PATH,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True, # Use True if DEVICE is 'cuda'\n",
        "        drop_last=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        persistent_workers=True if NUM_WORKERS > 0 else False # Can speed up epoch start\n",
        "    )\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=VAL_BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True, # Use True if DEVICE is 'cuda'\n",
        "        drop_last=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        persistent_workers=True if NUM_WORKERS > 0 else False # Can speed up epoch start\n",
        "    )\n",
        "\n",
        "print(\"DataLoader created.\")"
      ],
      "metadata": {
        "id": "nuF4Ph7D1igF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualize blur, sharp, and restored images\n",
        "Visualize few images from every validation batch to monitor the results of training"
      ],
      "metadata": {
        "id": "npqvOiAD05Y4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vwwluk5jAtl"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_validation_batch(blur_batch, pred_batch, sharp_batch, num_images=4, title_prefix=\"\"):\n",
        "\n",
        "    if not all(isinstance(t, torch.Tensor) for t in [blur_batch, pred_batch, sharp_batch]):\n",
        "        print(\"Warning: All input batches must be PyTorch Tensors.\")\n",
        "        return\n",
        "\n",
        "    if not (blur_batch.ndim == 4 and pred_batch.ndim == 4 and sharp_batch.ndim == 4):\n",
        "        print(\"Warning: All input batches must be 4D tensors (B, C, H, W).\")\n",
        "        return\n",
        "\n",
        "    num_to_show = min(num_images, blur_batch.shape[0], pred_batch.shape[0], sharp_batch.shape[0])\n",
        "\n",
        "    if num_to_show == 0:\n",
        "        print(\"Warning: No images to show (batch size might be 0, num_images=0, or mismatched batch sizes).\")\n",
        "        return\n",
        "\n",
        "    # Detach tensors from the computation graph and select the subset to show\n",
        "    blur_imgs_t = blur_batch[:num_to_show].detach()\n",
        "    pred_imgs_t = pred_batch[:num_to_show].detach()\n",
        "    sharp_imgs_t = sharp_batch[:num_to_show].detach()\n",
        "\n",
        "    # Clamp image values to [0, 1] for proper display\n",
        "    # (Important if model outputs are not strictly in this range)\n",
        "    blur_imgs_t = torch.clamp(blur_imgs_t, 0, 1)\n",
        "    pred_imgs_t = torch.clamp(pred_imgs_t, 0, 1)\n",
        "    sharp_imgs_t = torch.clamp(sharp_imgs_t, 0, 1)\n",
        "\n",
        "    # Create subplots: num_to_show rows, 3 columns (Input, Predicted, Ground Truth)\n",
        "    # Adjust figsize as needed\n",
        "    fig, axes = plt.subplots(num_to_show, 3, figsize=(12, num_to_show * 4))\n",
        "\n",
        "    # If num_to_show is 1, axes is a 1D array, so we need to handle it\n",
        "    if num_to_show == 1:\n",
        "        axes = axes.reshape(1, -1) # Reshape to (1, 3) to make indexing consistent\n",
        "\n",
        "    for i in range(num_to_show):\n",
        "        # Convert tensors to NumPy arrays for matplotlib\n",
        "        # Permute from (C, H, W) to (H, W, C) and move to CPU\n",
        "        blur_np = blur_imgs_t[i].cpu().permute(1, 2, 0).numpy()\n",
        "        pred_np = pred_imgs_t[i].cpu().permute(1, 2, 0).numpy()\n",
        "        sharp_np = sharp_imgs_t[i].cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "        # --- Column 0: Input Blurry ---\n",
        "        axes[i, 0].imshow(blur_np)\n",
        "        axes[i, 0].set_title(f\"Input Blurry {i+1}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # --- Column 1: Predicted Sharp ---\n",
        "        axes[i, 1].imshow(pred_np)\n",
        "        axes[i, 1].set_title(f\"Predicted Sharp {i+1}\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # --- Column 2: Ground Truth Sharp ---\n",
        "        axes[i, 2].imshow(sharp_np)\n",
        "        axes[i, 2].set_title(f\"Ground Truth Sharp {i+1}\")\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "    plt.suptitle(title_prefix, fontsize=14, y=1.0) # y=1.0 might be slightly high, adjust if needed\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.97]) # rect to make space for suptitle\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Validation Loss\n",
        "Functions for calculating validation loss every epoch while training the models\n",
        "\n",
        "\n",
        "\n",
        "- restormer\n",
        "- deepdeblur"
      ],
      "metadata": {
        "id": "IEOd6OVlxfPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation Logic for Restormer\n",
        "\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
        "import gc\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_epoch(model, dataloader, device, criterion, visualize=False, num_images_to_show=4, epoch_num=None):\n",
        "    model.eval()\n",
        "    # Initialize validation loss\n",
        "    val_loss = 0.0\n",
        "    visualized_this_epoch = not visualize\n",
        "    # Handle empty dataloader\n",
        "    if not dataloader:\n",
        "        print(\"Validation dataloader is empty.\")\n",
        "        model.train()\n",
        "        return 0.0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Validating Epoch {epoch_num if epoch_num is not None else 'N/A'}\", leave=False)\n",
        "    for batch_idx, (blur_imgs, sharp_imgs) in enumerate(progress_bar):\n",
        "        blur_imgs = blur_imgs.to(device)\n",
        "        sharp_imgs = sharp_imgs.to(device)\n",
        "\n",
        "        recover_img = model(blur_imgs)\n",
        "\n",
        "        if not visualized_this_epoch:\n",
        "            print(f\"\\nVisualizing Validation Batch {batch_idx} (Epoch {epoch_num if epoch_num is not None else 'N/A'})...\")\n",
        "            title = f\"Validation - Epoch {epoch_num}\" if epoch_num is not None else \"Validation\"\n",
        "            visualize_validation_batch(blur_imgs.cpu(),\n",
        "                                       recover_img.cpu(),\n",
        "                                       sharp_imgs.cpu(),\n",
        "                                       num_images=num_images_to_show,\n",
        "                                       title_prefix=title)\n",
        "            visualized_this_epoch = True # Ensure visualization happens only once per epoch call\n",
        "\n",
        "        # Uses ReconstructLoss's forward\n",
        "        losses = criterion(recover_img, sharp_imgs)\n",
        "        grad_loss = losses[\"total_loss\"]\n",
        "\n",
        "        current_batch_loss = grad_loss.item()\n",
        "        val_loss += current_batch_loss\n",
        "\n",
        "        # Display current batch loss and running average epoch loss\n",
        "        progress_bar.set_postfix(\n",
        "            BatchLoss=f\"{current_batch_loss:.4f}\",\n",
        "            AvgEpochLoss=f\"{val_loss / (batch_idx + 1):.4f}\"\n",
        "        )\n",
        "\n",
        "        del recover_img, losses, grad_loss, current_batch_loss\n",
        "\n",
        "\n",
        "    if len(dataloader) > 0:\n",
        "        avg_val_loss = val_loss / len(dataloader)\n",
        "    else:\n",
        "        avg_val_loss = 0.0\n",
        "\n",
        "    try:\n",
        "        del blur_imgs, sharp_imgs\n",
        "    except NameError:\n",
        "        pass\n",
        "    gc.collect()\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return avg_val_loss"
      ],
      "metadata": {
        "id": "Y2BaQztr5dI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS9K9XJ647Je"
      },
      "outputs": [],
      "source": [
        "#Validation Logic for DeepDeblur\n",
        "\n",
        "\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
        "import gc\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_epoch(model, dataloader, device, criterion, visualize=False, num_images_to_show=4, epoch_num=None):\n",
        "    model.eval()\n",
        "    # Initialize validation loss\n",
        "    val_loss = 0.0\n",
        "    visualized_this_epoch = not visualize\n",
        "    # Handle empty dataloader\n",
        "    if not dataloader:\n",
        "        print(\"Validation dataloader is empty.\")\n",
        "        model.train()\n",
        "        return 0.0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Validating Epoch {epoch_num if epoch_num is not None else 'N/A'}\", leave=False)\n",
        "    for batch_idx, (blur_imgs, sharp_imgs) in enumerate(progress_bar):\n",
        "        blur_imgs = blur_imgs.to(device)\n",
        "        sharp_imgs = sharp_imgs.to(device)\n",
        "\n",
        "        sharp_gt_half = F.interpolate(sharp_imgs, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "        sharp_gt_quarter = F.interpolate(sharp_gt_half, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "\n",
        "        fine_out, mid_out, coarse_out = model(blur_imgs)\n",
        "\n",
        "        sharp_gt_half_224 = F.interpolate(sharp_gt_half, size = 224, mode='bilinear', align_corners=False)\n",
        "        sharp_gt_quarter_224 = F.interpolate(sharp_gt_quarter, size = 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "        mid_out_224 = F.interpolate(mid_out, size = 224, mode='bilinear', align_corners=False)\n",
        "        coarse_out_224 = F.interpolate(coarse_out, size = 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "        if not visualized_this_epoch:\n",
        "            print(f\"\\nVisualizing Validation Batch {batch_idx} (Epoch {epoch_num if epoch_num is not None else 'N/A'})...\")\n",
        "            title = f\"Validation - Epoch {epoch_num}\" if epoch_num is not None else \"Validation\"\n",
        "            visualize_validation_batch(blur_imgs.cpu(),\n",
        "                                       fine_out.cpu(),\n",
        "                                       sharp_imgs.cpu(),\n",
        "                                       num_images=num_images_to_show,\n",
        "                                       title_prefix=title)\n",
        "            visualized_this_epoch = True # Ensure visualization happens only once per epoch call\n",
        "\n",
        "        # Uses ReconstructLoss's forward\n",
        "        loss_fine = criterion(fine_out, sharp_imgs)\n",
        "        loss_mid = criterion(mid_out_224, sharp_gt_half_224)\n",
        "        loss_coarse = criterion(coarse_out_224, sharp_gt_quarter_224)\n",
        "        # Combining the fine, mid and coarse losses\n",
        "        losses = loss_fine[\"total_loss\"] + loss_mid[\"total_loss\"] + loss_coarse[\"total_loss\"]\n",
        "\n",
        "        current_batch_loss = losses.item()\n",
        "        val_loss += current_batch_loss\n",
        "\n",
        "        # Display current batch loss and running average epoch loss\n",
        "        progress_bar.set_postfix(\n",
        "            BatchLoss=f\"{current_batch_loss:.4f}\",\n",
        "            AvgEpochLoss=f\"{val_loss / (batch_idx + 1):.4f}\"\n",
        "        )\n",
        "\n",
        "        del fine_out, mid_out, coarse_out, mid_out_224, coarse_out_224, sharp_gt_half, sharp_gt_quarter, sharp_gt_half_224, sharp_gt_quarter_224\n",
        "\n",
        "\n",
        "    if len(dataloader) > 0:\n",
        "        avg_val_loss = val_loss / len(dataloader)\n",
        "    else:\n",
        "        avg_val_loss = 0.0\n",
        "\n",
        "    try:\n",
        "        del blur_imgs, sharp_imgs\n",
        "    except NameError:\n",
        "        pass\n",
        "    gc.collect()\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return avg_val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ANotA0t03bYt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-V45465t_Yc"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEy_zpa67ZfO"
      },
      "source": [
        "##Setting up Restormer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ChdMuuBAIH5"
      },
      "outputs": [],
      "source": [
        "!pip install einops\n",
        "\n",
        "if os.path.isdir('Restormer'):\n",
        "  !rm -r Restormer\n",
        "\n",
        "# Clone Restormer\n",
        "!git clone https://github.com/swz30/Restormer.git\n",
        "%cd Restormer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/Restormer')"
      ],
      "metadata": {
        "id": "93semyl3263Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJtmVds-BQom"
      },
      "outputs": [],
      "source": [
        "# task = 'Real_Denoising'\n",
        "task = 'Single_Image_Defocus_Deblurring'\n",
        "# task = 'Motion_Deblurring'\n",
        "# task = 'Deraining'\n",
        "\n",
        "# Download the pre-trained models\n",
        "if task is 'Real_Denoising':\n",
        "  !wget https://github.com/swz30/Restormer/releases/download/v1.0/real_denoising.pth -P Denoising/pretrained_models\n",
        "if task is 'Single_Image_Defocus_Deblurring':\n",
        "  !wget https://github.com/swz30/Restormer/releases/download/v1.0/single_image_defocus_deblurring.pth -P Defocus_Deblurring/pretrained_models\n",
        "if task is 'Motion_Deblurring':\n",
        "  !wget https://github.com/swz30/Restormer/releases/download/v1.0/motion_deblurring.pth -P Motion_Deblurring/pretrained_models\n",
        "if task is 'Deraining':\n",
        "  !wget https://github.com/swz30/Restormer/releases/download/v1.0/deraining.pth -P Deraining/pretrained_models\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Pretrained Restormer\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "from runpy import run_path\n",
        "from skimage import img_as_ubyte\n",
        "from natsort import natsorted\n",
        "from glob import glob\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from basicsr.models.archs.restormer_arch import Restormer\n",
        "\n",
        "def get_weights_and_parameters(task, parameters):\n",
        "    if task == 'Motion_Deblurring':\n",
        "        weights = os.path.join('Motion_Deblurring', 'pretrained_models', 'motion_deblurring.pth')\n",
        "    elif task == 'Single_Image_Defocus_Deblurring':\n",
        "        weights = os.path.join('Defocus_Deblurring', 'pretrained_models', 'single_image_defocus_deblurring.pth')\n",
        "    elif task == 'Deraining':\n",
        "        weights = os.path.join('Deraining', 'pretrained_models', 'deraining.pth')\n",
        "    elif task == 'Real_Denoising':\n",
        "        weights = os.path.join('Denoising', 'pretrained_models', 'real_denoising.pth')\n",
        "        parameters['LayerNorm_type'] =  'BiasFree'\n",
        "    return weights, parameters\n",
        "\n",
        "\n",
        "\n",
        "# use pretrained restormer's weights\n",
        "\n",
        "#model_path = '/content/Restormer/Defocus_Deblurring/pretrained_models/single_image_defocus_deblurring.pth'\n",
        "\n",
        "# checkpoint = torch.load(model_path, map_location='cpu')\n",
        "# try:\n",
        "#     model.load_state_dict(checkpoint[\"params\"], strict=True)\n",
        "#     print(f\"Loaded weights from {model_path} using 'params' key.\")\n",
        "# except KeyError:\n",
        "#     try:\n",
        "#         model.load_state_dict(checkpoint, strict=True)\n",
        "#         print(f\"Loaded weights directly from {model_path}.\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error loading state dict: {e}\")\n",
        "#         print(\"You might need to inspect the checkpoint keys or adjust loading logic.\")\n"
      ],
      "metadata": {
        "id": "xKpIN8zy3qV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0bbEbRZjZ0n"
      },
      "source": [
        "##Setting up DeepDeblur model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qbOw2whjVa7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ---------------------------------------\n",
        "# Residual Block\n",
        "# ---------------------------------------\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, num_feats):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(num_feats, num_feats, 3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(num_feats, num_feats, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv2(self.relu(self.conv1(x)))\n",
        "\n",
        "# ---------------------------------------\n",
        "# Single-Scale Deblurring Network\n",
        "# ---------------------------------------\n",
        "class SingleScaleDeblurNet(nn.Module):\n",
        "    def __init__(self, in_channels, num_feats=64, num_blocks=8):\n",
        "        super().__init__()\n",
        "        self.head = nn.Conv2d(in_channels, num_feats, kernel_size=3, padding=1)\n",
        "        self.body = nn.Sequential(*[ResBlock(num_feats) for _ in range(num_blocks)])\n",
        "        self.tail = nn.Conv2d(num_feats, 3, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.head(x)\n",
        "        feat = self.body(feat)\n",
        "        out = self.tail(feat)\n",
        "        return out\n",
        "\n",
        "# ---------------------------------------\n",
        "# Multi-Scale Deblurring Network (DeepDeblurMS)\n",
        "# ---------------------------------------\n",
        "class DeepDeblurMS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Each stage expects concatenated inputs → 6 channels: [blur, upsampled_output]\n",
        "        self.coarse_net = SingleScaleDeblurNet(in_channels=6)\n",
        "        self.middle_net = SingleScaleDeblurNet(in_channels=6)\n",
        "        self.fine_net = SingleScaleDeblurNet(in_channels=6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Create image pyramid\n",
        "        x_half = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "        x_quarter = F.interpolate(x_half, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Coarse scale input: duplicate x_quarter to simulate [blur, blur]\n",
        "        coarse_input = torch.cat([x_quarter, x_quarter], dim=1)\n",
        "        coarse_out = self.coarse_net(coarse_input)\n",
        "        up_coarse = F.interpolate(coarse_out, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Middle scale input: [blur_half, upsampled_coarse]\n",
        "        mid_input = torch.cat([x_half, up_coarse], dim=1)\n",
        "        mid_out = self.middle_net(mid_input)\n",
        "        up_mid = F.interpolate(mid_out, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Fine scale input: [blur_full, upsampled_middle]\n",
        "        fine_input = torch.cat([x, up_mid], dim=1)\n",
        "        fine_out = self.fine_net(fine_input)\n",
        "\n",
        "        return fine_out, mid_out, coarse_out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7p3_yJaiId8"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKAHv4WV6iBj"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from natsort import natsorted\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from pdb import set_trace as stx\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from functools import partial\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4bvSCz9pTWf"
      },
      "outputs": [],
      "source": [
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training logic for Restormer model"
      ],
      "metadata": {
        "id": "B-tIt9Kp15II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Restormer on Perceptual Loss\n",
        "\n",
        "import loss.deblur_loss as deblur_loss\n",
        "from deblur_loss import ReconstructPerceptualLoss as ReconstructLoss\n",
        "import importlib\n",
        "from utils import pos_embed\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.vision_transformer import PatchEmbed, Block as TimmBlock\n",
        "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
        "from utils.pos_embed import get_2d_sincos_pos_embed\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import gc\n",
        "\n",
        "LOAD_PRETRAINED = True\n",
        "run_validation = True\n",
        "\n",
        "MODEL_NAME = 'Restormer'\n",
        "\n",
        "# Restormer for deblur task\n",
        "\n",
        "parameters = {'inp_channels':3, 'out_channels':3, 'dim':48, 'num_blocks':[4,6,6,8], 'num_refinement_blocks':4, 'heads':[1,2,4,8], 'ffn_expansion_factor':2.66, 'bias':False, 'LayerNorm_type':'WithBias', 'dual_pixel_task':False}\n",
        "\n",
        "model = Restormer(**parameters)\n",
        "\n",
        "try:\n",
        "    opt = {'image_size': 224, 'pretrain_mae': PRETRAINED_WEIGHTS_PATH, 'device': DEVICE}\n",
        "    criterion = ReconstructLoss(opt)\n",
        "    model = model.cuda()\n",
        "    criterion.pretrain_mae = criterion.pretrain_mae.to(torch.device('cuda'))\n",
        "    print(\"Custom loss criterion initialized.\")\n",
        "except Exception as e: raise SystemExit(f\"Error initializing loss: {e}\")\n",
        "\n",
        "effective_lr = 5e-5 if LOAD_PRETRAINED and os.path.exists(PRETRAINED_WEIGHTS_PATH) else LEARNING_RATE\n",
        "optimizer = optim.Adam(model.parameters(), lr=effective_lr, weight_decay=WEIGHT_DECAY)\n",
        "print(f\"Optimizer: Adam, LR: {effective_lr:.1e}, Weight Decay: {WEIGHT_DECAY:.1e}\")\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    num_halvings = epoch // LR_DECAY_EPOCHS\n",
        "    lr_multiplier = 0.5 ** num_halvings\n",
        "    final_multiplier = LR_FINAL / effective_lr # Use the actual starting LR\n",
        "    return max(lr_multiplier, final_multiplier)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "print(f\"LR Scheduler: Halve every {LR_DECAY_EPOCHS} epochs, min LR {LR_FINAL:.1e}\")\n",
        "\n",
        "\n",
        "print(f\"\\n--- Starting Training for {EPOCHS} Epochs ---\")\n",
        "start_time = time.time()\n",
        "best_val_loss = 0.0\n",
        "SAVE_BEST_PATH = os.path.join(OUTPUT_DIR, f\"{MODEL_NAME}_best_restormer_new.pth\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss_total = 0.0\n",
        "    epoch_loss_l1 = 0.0\n",
        "    epoch_loss_perc = 0.0\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
        "\n",
        "    for batch_idx, (blur_imgs, sharp_imgs) in enumerate(progress_bar):\n",
        "\n",
        "        gt_img = sharp_imgs.to(DEVICE)\n",
        "        b_img = blur_imgs.to(DEVICE)\n",
        "        recover_img = model(b_img)\n",
        "        losses = criterion(recover_img, gt_img)\n",
        "\n",
        "        grad_loss = losses[\"total_loss\"]\n",
        "        optimizer.zero_grad()\n",
        "        grad_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss_total += grad_loss.item()\n",
        "        epoch_loss_l1 += losses.get('l1', torch.tensor(0.0)).item()\n",
        "        epoch_loss_perc += losses.get('Perceptual', torch.tensor(0.0)).item()\n",
        "        progress_bar.set_postfix(loss=f\"{grad_loss.item():.4f}\")\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_loss_total = epoch_loss_total / len(dataloader)\n",
        "    avg_loss_l1 = epoch_loss_l1 / len(dataloader)\n",
        "    avg_loss_perc = epoch_loss_perc / len(dataloader)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    print(\"Clearing cache before validation...\")\n",
        "    del gt_img, b_img, blur_imgs, sharp_imgs, gt_img, b_img, recover_img, losses, grad_loss\n",
        "    gc.collect()\n",
        "    if DEVICE == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if run_validation and val_dataloader:\n",
        "\n",
        "        val_loss = validate_epoch(model, val_dataloader, DEVICE, criterion, visualize=True, num_images_to_show=4, epoch_num=None)\n",
        "\n",
        "        if val_loss > best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), SAVE_BEST_PATH)\n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f}) || Validation Loss: {val_loss:.4f} *** Best Model Saved ***\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f}) || Validation Loss: {val_loss:.4f}\")\n",
        "    else:\n",
        "         print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f}) || Validation SKIPPED\")\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f})\")\n",
        "\n",
        "    current_checkpoint_path = os.path.join(OUTPUT_DIR, f\"checkpoint_epoch_{epoch}.pth\")\n",
        "\n",
        "    save_data = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': avg_loss_total,\n",
        "        'val_loss': val_loss, }\n",
        "    torch.save(save_data, current_checkpoint_path)\n",
        "\n",
        "status = \"Transferred weights\" if LOAD_PRETRAINED and os.path.exists(PRETRAINED_WEIGHTS_PATH) else \"scratch\"\n",
        "final_model_path = os.path.join(OUTPUT_DIR, f\"{status}_customloss_restormer_epoch{EPOCHS}_final.pth\")\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "total_time = time.time() - start_time\n",
        "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "print(f\"\\n--- Training Finished ---\")\n",
        "print(f\"Total Training Time: {total_time_str}\")\n",
        "print(f\"Final model saved to: {final_model_path}\")\n",
        "if run_validation and os.path.exists(SAVE_BEST_PATH):\n",
        "    print(f\"Best model (Validation PSNR: {best_val_loss:.4f}) saved to: {SAVE_BEST_PATH}\")\n",
        "elif run_validation:\n",
        "    print(f\"Best model not saved (Validation PSNR did not improve beyond initial {best_val_loss:.4f}).\")\n",
        "else:\n",
        "    print(\"Best model not saved (Validation was skipped).\")\n",
        "\n",
        "\n",
        "if 'dataset' in locals() and hasattr(dataset, 'close'): dataset.close()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "8gawrDpkV_V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1XNpFq3B3dM"
      },
      "source": [
        "##Training Logic for DeepDeblur model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T896BMtNUXbj"
      },
      "outputs": [],
      "source": [
        "#Training DeepDeblur on Perceptual Loss\n",
        "\n",
        "import loss.deblur_loss as deblur_loss\n",
        "from deblur_loss import ReconstructPerceptualLoss as ReconstructLoss\n",
        "import importlib\n",
        "from utils import pos_embed\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.vision_transformer import PatchEmbed, Block as TimmBlock\n",
        "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
        "from utils.pos_embed import get_2d_sincos_pos_embed\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import gc\n",
        "\n",
        "LOAD_PRETRAINED = True\n",
        "run_validation = True\n",
        "\n",
        "model = DeepDeblurMS()\n",
        "\n",
        "try:\n",
        "    opt = {'image_size': IMG_SIZE, 'pretrain_mae': PRETRAINED_WEIGHTS_PATH, 'device': DEVICE}\n",
        "    criterion = ReconstructLoss(opt)\n",
        "    model = model.cuda()\n",
        "    criterion.pretrain_mae = criterion.pretrain_mae.to(torch.device('cuda'))\n",
        "    print(\"Custom loss criterion initialized.\")\n",
        "except Exception as e: raise SystemExit(f\"Error initializing loss: {e}\")\n",
        "\n",
        "effective_lr = 5e-5 if LOAD_PRETRAINED and os.path.exists(PRETRAINED_WEIGHTS_PATH) else LEARNING_RATE\n",
        "optimizer = optim.Adam(model.parameters(), lr=effective_lr, weight_decay=WEIGHT_DECAY)\n",
        "print(f\"Optimizer: Adam, LR: {effective_lr:.1e}, Weight Decay: {WEIGHT_DECAY:.1e}\")\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    num_halvings = epoch // LR_DECAY_EPOCHS\n",
        "    lr_multiplier = 0.5 ** num_halvings\n",
        "    final_multiplier = LR_FINAL / effective_lr # Use the actual starting LR\n",
        "    return max(lr_multiplier, final_multiplier)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "print(f\"LR Scheduler: Halve every {LR_DECAY_EPOCHS} epochs, min LR {LR_FINAL:.1e}\")\n",
        "\n",
        "\n",
        "print(f\"\\n--- Starting Training for {EPOCHS} Epochs ---\")\n",
        "start_time = time.time()\n",
        "best_val_loss = 0.0\n",
        "SAVE_BEST_PATH = os.path.join(OUTPUT_DIR, f\"{MODEL_NAME}_best_deepDeblur.pth\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss_total = 0.0\n",
        "    epoch_loss_l1 = 0.0\n",
        "    epoch_loss_perc = 0.0\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
        "\n",
        "    for batch_idx, (blur_imgs, sharp_imgs) in enumerate(progress_bar):\n",
        "\n",
        "        gt_img = sharp_imgs.to(DEVICE)\n",
        "        b_img = blur_imgs.to(DEVICE)\n",
        "\n",
        "        sharp_gt_half = F.interpolate(gt_img, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "        sharp_gt_quarter = F.interpolate(sharp_gt_half, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "\n",
        "        fine_out, mid_out, coarse_out = model(b_img)\n",
        "\n",
        "        sharp_gt_half_224 = F.interpolate(sharp_gt_half, size = 224, mode='bilinear', align_corners=False)\n",
        "        sharp_gt_quarter_224 = F.interpolate(sharp_gt_quarter, size = 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "        mid_out_224 = F.interpolate(mid_out, size = 224, mode='bilinear', align_corners=False)\n",
        "        coarse_out_224 = F.interpolate(coarse_out, size = 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Uses ReconstructLoss's forward\n",
        "        loss_fine = criterion(fine_out, gt_img)\n",
        "        loss_mid = criterion(mid_out_224, sharp_gt_half_224)\n",
        "        loss_coarse = criterion(coarse_out_224, sharp_gt_quarter_224)\n",
        "        # Combining the fine, mid and coarse losses\n",
        "        losses = loss_fine[\"total_loss\"] + loss_mid[\"total_loss\"] + loss_coarse[\"total_loss\"]\n",
        "\n",
        "        # 4. Backpropagation\n",
        "        grad_loss = loss_fine[\"total_loss\"] + loss_mid[\"total_loss\"] + loss_coarse[\"total_loss\"]\n",
        "        optimizer.zero_grad()\n",
        "        grad_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Logging\n",
        "        epoch_loss_total += grad_loss.item()\n",
        "        epoch_loss_l1 += loss_fine.get('l1', torch.tensor(0.0)).item() + loss_mid.get('l1', torch.tensor(0.0)).item() + loss_coarse.get('l1', torch.tensor(0.0)).item()\n",
        "        epoch_loss_perc += loss_fine.get('Perceptual', torch.tensor(0.0)).item() + loss_mid.get('Perceptual', torch.tensor(0.0)).item() + loss_coarse.get('Perceptual', torch.tensor(0.0)).item()\n",
        "        progress_bar.set_postfix(loss=f\"{grad_loss.item():.4f}\")\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_loss_total = epoch_loss_total / len(dataloader)\n",
        "    avg_loss_l1 = epoch_loss_l1 / len(dataloader)\n",
        "    avg_loss_perc = epoch_loss_perc / len(dataloader)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    print(\"Clearing cache before validation...\")\n",
        "    del blur_imgs, sharp_imgs, gt_img, b_img, sharp_gt_half, sharp_gt_quarter, fine_out, mid_out, coarse_out, sharp_gt_half_224, sharp_gt_quarter_224, mid_out_224, coarse_out_224, losses, grad_loss\n",
        "    gc.collect()\n",
        "    if DEVICE == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if run_validation and val_dataloader:\n",
        "\n",
        "        val_loss = validate_epoch(model, val_dataloader, DEVICE, criterion, visualize=True, num_images_to_show=4, epoch_num=None)\n",
        "\n",
        "        if val_loss > best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), SAVE_BEST_PATH)\n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f}) || Validation Loss: {val_loss:.4f} *** Best Model Saved ***\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f}) || Validation Loss: {val_loss:.4f}\")\n",
        "    else:\n",
        "         print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f}) || Validation SKIPPED\")\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f})\")\n",
        "\n",
        "    current_checkpoint_path = os.path.join(OUTPUT_DIR, f\"checkpoint_epoch_{epoch}.pth\")\n",
        "\n",
        "    save_data = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': avg_loss_total,\n",
        "        'val_loss': val_loss, }\n",
        "    torch.save(save_data, current_checkpoint_path)\n",
        "\n",
        "status = \"Transferred weights\" if LOAD_PRETRAINED and os.path.exists(PRETRAINED_WEIGHTS_PATH) else \"scratch\"\n",
        "final_model_path = os.path.join(OUTPUT_DIR, f\"{MODEL_NAME}_{status}_customloss_deepDeblur_epoch{EPOCHS}_final.pth\")\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "total_time = time.time() - start_time\n",
        "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "print(f\"\\n--- Training Finished ---\")\n",
        "print(f\"Total Training Time: {total_time_str}\")\n",
        "print(f\"Final model saved to: {final_model_path}\")\n",
        "if run_validation and os.path.exists(SAVE_BEST_PATH):\n",
        "    print(f\"Best model (Validation PSNR: {best_val_loss:.4f}) saved to: {SAVE_BEST_PATH}\")\n",
        "elif run_validation:\n",
        "    print(f\"Best model not saved (Validation PSNR did not improve beyond initial {best_val_loss:.4f}).\")\n",
        "else:\n",
        "    print(\"Best model not saved (Validation was skipped).\")\n",
        "\n",
        "\n",
        "if 'dataset' in locals() and hasattr(dataset, 'close'): dataset.close()\n",
        "print(\"Done.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "L2K4drBNtrqe",
        "jRncBPYEbATz"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "18LQLAAtDKDJPSeNms8NGf3FqJ9FhikoZ",
      "authorship_tag": "ABX9TyMhjdvjbRWND+LKM6rhQAI3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}